{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:42:04.136272Z",
     "iopub.status.busy": "2023-06-10T10:42:04.135517Z",
     "iopub.status.idle": "2023-06-10T10:42:04.161864Z",
     "shell.execute_reply": "2023-06-10T10:42:04.160872Z",
     "shell.execute_reply.started": "2023-06-10T10:42:04.136159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "MAX_LENGTH = 512\n",
    "MODEL_PATH = \"csebuetnlp/banglishbert\" # choices: csebuetnlp/banglabert, csebuetnlp/banglishbert, bert-base-multilingual-cased\n",
    "DATA_PATH = \"./Dataset/\"\n",
    "DROPOUT = 0.4\n",
    "WEIGHT_DECAY = 3e-2\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:06.027443Z",
     "iopub.status.busy": "2023-06-10T10:43:06.027030Z",
     "iopub.status.idle": "2023-06-10T10:43:16.032962Z",
     "shell.execute_reply": "2023-06-10T10:43:16.031792Z",
     "shell.execute_reply.started": "2023-06-10T10:43:06.027397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:16.037763Z",
     "iopub.status.busy": "2023-06-10T10:43:16.036195Z",
     "iopub.status.idle": "2023-06-10T10:43:16.755096Z",
     "shell.execute_reply": "2023-06-10T10:43:16.754092Z",
     "shell.execute_reply.started": "2023-06-10T10:43:16.037723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict(pd.read_json(f\"{DATA_PATH}train.json\")),\n",
    "    'valid': Dataset.from_dict(pd.read_json(f\"{DATA_PATH}valid.json\")),\n",
    "    'test': Dataset.from_dict(pd.read_json(f\"{DATA_PATH}test.json\"))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:16.757161Z",
     "iopub.status.busy": "2023-06-10T10:43:16.756777Z",
     "iopub.status.idle": "2023-06-10T10:43:16.763036Z",
     "shell.execute_reply": "2023-06-10T10:43:16.761843Z",
     "shell.execute_reply.started": "2023-06-10T10:43:16.757121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes = [\"O\",\"B-Symptom\",\"I-Symptom\",\"B-Health Condition\",\"I-Health Condition\",\"B-Age\",\"I-Age\",\"B-Medicine\",\"I-Medicine\",\"B-Dosage\",\"I-Dosage\",\"B-Medical Procedure\",\"I-Medical Procedure\",\"B-Specialist\",\"I-Specialist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:16.765432Z",
     "iopub.status.busy": "2023-06-10T10:43:16.764753Z",
     "iopub.status.idle": "2023-06-10T10:43:16.773090Z",
     "shell.execute_reply": "2023-06-10T10:43:16.772093Z",
     "shell.execute_reply.started": "2023-06-10T10:43:16.765394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_ids = {k: v for v, k in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:16.775311Z",
     "iopub.status.busy": "2023-06-10T10:43:16.774915Z",
     "iopub.status.idle": "2023-06-10T10:43:16.783073Z",
     "shell.execute_reply": "2023-06-10T10:43:16.781979Z",
     "shell.execute_reply.started": "2023-06-10T10:43:16.775274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature = ClassLabel(num_classes=len(classes), names=classes)\n",
    "ds['train'].features['labels'].feature = feature\n",
    "ds['test'].features['labels'].feature = feature\n",
    "ds['valid'].features['labels'].feature = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:16.785033Z",
     "iopub.status.busy": "2023-06-10T10:43:16.784567Z",
     "iopub.status.idle": "2023-06-10T10:43:20.595973Z",
     "shell.execute_reply": "2023-06-10T10:43:20.594917Z",
     "shell.execute_reply.started": "2023-06-10T10:43:16.784993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:20.598080Z",
     "iopub.status.busy": "2023-06-10T10:43:20.597675Z",
     "iopub.status.idle": "2023-06-10T10:43:20.608453Z",
     "shell.execute_reply": "2023-06-10T10:43:20.607318Z",
     "shell.execute_reply.started": "2023-06-10T10:43:20.598026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(samples):\n",
    "    samples['text'] = [sample.split() for sample in samples['text']]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        samples[\"text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(samples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(class_ids[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:20.613511Z",
     "iopub.status.busy": "2023-06-10T10:43:20.613213Z",
     "iopub.status.idle": "2023-06-10T10:43:30.823977Z",
     "shell.execute_reply": "2023-06-10T10:43:30.823029Z",
     "shell.execute_reply.started": "2023-06-10T10:43:20.613485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:30.830840Z",
     "iopub.status.busy": "2023-06-10T10:43:30.828166Z",
     "iopub.status.idle": "2023-06-10T10:43:30.837679Z",
     "shell.execute_reply": "2023-06-10T10:43:30.836531Z",
     "shell.execute_reply.started": "2023-06-10T10:43:30.830795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:30.843419Z",
     "iopub.status.busy": "2023-06-10T10:43:30.841601Z",
     "iopub.status.idle": "2023-06-10T10:43:31.720317Z",
     "shell.execute_reply": "2023-06-10T10:43:31.719308Z",
     "shell.execute_reply.started": "2023-06-10T10:43:30.843381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:31.723326Z",
     "iopub.status.busy": "2023-06-10T10:43:31.721764Z",
     "iopub.status.idle": "2023-06-10T10:43:31.732512Z",
     "shell.execute_reply": "2023-06-10T10:43:31.731170Z",
     "shell.execute_reply.started": "2023-06-10T10:43:31.723283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [classes[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [classes[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division = 0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:31.734552Z",
     "iopub.status.busy": "2023-06-10T10:43:31.734116Z",
     "iopub.status.idle": "2023-06-10T10:43:45.208120Z",
     "shell.execute_reply": "2023-06-10T10:43:45.206974Z",
     "shell.execute_reply.started": "2023-06-10T10:43:31.734501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "configuration = AutoConfig.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(classes),\n",
    "    id2label={index: label for index, label in enumerate(classes)},\n",
    "    label2id={label: index for index, label in enumerate(classes)},\n",
    ")\n",
    "configuration.hidden_dropout_prob = DROPOUT\n",
    "configuration.attention_probs_dropout_prob = DROPOUT\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    config=configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Calculate warmup_steps ensuring it is an integer and >= 1\n",
    "total_steps = int(len(ds[\"train\"]) / BATCH_SIZE * EPOCHS)\n",
    "warmup_steps = max(1, int(0.2 * total_steps))  # Ensuring warmup_steps is at least 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:43:45.210259Z",
     "iopub.status.busy": "2023-06-10T10:43:45.209783Z",
     "iopub.status.idle": "2023-06-10T10:46:38.254754Z",
     "shell.execute_reply": "2023-06-10T10:46:38.253301Z",
     "shell.execute_reply.started": "2023-06-10T10:43:45.210218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=warmup_steps,  # Ensure this is >= 1\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir=\"./best_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T10:46:38.257713Z",
     "iopub.status.busy": "2023-06-10T10:46:38.257254Z",
     "iopub.status.idle": "2023-06-10T10:47:25.984933Z",
     "shell.execute_reply": "2023-06-10T10:47:25.983013Z",
     "shell.execute_reply.started": "2023-06-10T10:46:38.257667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_ds[\"test\"]).predictions\n",
    "labels = tokenized_ds['test']['labels']\n",
    "compute_metrics((predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [classes[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "ds['text'] = ds['test'].add_column('preds', true_predictions)\n",
    "ds['test'].to_pandas().to_json('output.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def fix_json_format(content):\n",
    "    \"\"\"\n",
    "    Attempt to fix common JSON formatting issues.\n",
    "    \"\"\"\n",
    "    # Replace single quotes with double quotes\n",
    "    content = content.replace(\"'\", '\"')\n",
    "\n",
    "    # Fix trailing commas in arrays and objects\n",
    "    content = re.sub(r',\\s*([}\\]])', r'\\1', content)\n",
    "\n",
    "    # Fix missing double quotes around keys\n",
    "    content = re.sub(r'(\\w+):', r'\"\\1\":', content)\n",
    "\n",
    "    return content\n",
    "\n",
    "def extract_json_objects(content):\n",
    "    \"\"\"\n",
    "    Extract individual JSON objects from the content.\n",
    "    \"\"\"\n",
    "    # Find potential JSON objects by matching brackets\n",
    "    json_objects = re.findall(r'\\{(?:[^{}]|(?R))*\\}', content, re.DOTALL)\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def convert_json_to_format(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            content = infile.read()\n",
    "\n",
    "        # Attempt to fix common JSON formatting issues\n",
    "        fixed_content = fix_json_format(content)\n",
    "\n",
    "        # Extract individual JSON objects\n",
    "        json_objects = extract_json_objects(fixed_content)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for json_obj in json_objects:\n",
    "            try:\n",
    "                # Try to parse each JSON object\n",
    "                data = json.loads(json_obj)\n",
    "\n",
    "                if isinstance(data, dict):\n",
    "                    # Process the text by removing '\\n'\n",
    "                    text = ' '.join(token for token in data.get('tokens', []) if token != '\\n')\n",
    "\n",
    "                    # Create the output format\n",
    "                    output = {\n",
    "                        \"text\": text,\n",
    "                        \"labels\": data.get('tags', [])\n",
    "                    }\n",
    "\n",
    "                    results.append(output)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON object: {e}\")\n",
    "\n",
    "        # Save to the output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(results, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Conversion successful! Output saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: unknown extension ?R at position 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "input_file = 'Dataset/data.json'   # Replace with your input file path\n",
    "output_file = 'output.json' # Replace with your desired output file path\n",
    "convert_json_to_format(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
